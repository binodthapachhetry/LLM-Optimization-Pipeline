"""
Diagnostic utilities for troubleshooting LLM optimization issues.
"""

import logging
import json
from typing import Any, Dict, List, Optional, Union, Tuple
import torch

logger = logging.getLogger(__name__)

def log_completion_evaluation(
    prompt: str,
    expected_completion: str,
    generated_text: str,
    extracted_completion: str,
    is_match: bool,
    model_name: str,
    additional_info: Optional[Dict[str, Any]] = None
) -> None:
    """
    Log detailed information about a completion evaluation for debugging.
    
    Args:
        prompt: The input prompt
        expected_completion: The expected completion
        generated_text: The raw text generated by the model
        extracted_completion: The extracted completion from the generated text
        is_match: Whether the completion matched the expected completion
        model_name: Name or identifier of the model being evaluated
        additional_info: Any additional information to log
    """
    log_data = {
        "model": model_name,
        "prompt": prompt,
        "expected_completion": expected_completion,
        "generated_text": generated_text,
        "extracted_completion": extracted_completion,
        "is_match": is_match
    }
    
    if additional_info:
        log_data.update(additional_info)
    
    # Log as both structured JSON and human-readable format
    logger.info(f"COMPLETION EVALUATION: {json.dumps(log_data, indent=2)}")
    
    # Also log in a more readable format
    logger.info(f"\n{'='*80}\nCOMPLETION EVALUATION FOR {model_name}\n{'='*80}")
    logger.info(f"PROMPT: \"{prompt}\"")
    logger.info(f"EXPECTED: \"{expected_completion}\"")
    logger.info(f"GENERATED: \"{generated_text}\"")
    logger.info(f"EXTRACTED: \"{extracted_completion}\"")
    logger.info(f"MATCH: {is_match}")
    if additional_info:
        logger.info(f"ADDITIONAL INFO: {additional_info}")
    logger.info(f"{'='*80}\n")

def test_model_completion(
    model: Any,
    tokenizer: Any,
    prompt: str,
    expected_completion: Optional[str] = None,
    max_new_tokens: int = 50,
    model_name: str = "unknown"
) -> Dict[str, Any]:
    """
    Test a model's completion ability on a single prompt.
    
    Args:
        model: The model to test
        tokenizer: The tokenizer for the model
        prompt: The input prompt
        expected_completion: The expected completion (optional)
        max_new_tokens: Maximum number of tokens to generate
        model_name: Name or identifier of the model
        
    Returns:
        Dictionary with test results
    """
    logger.info(f"Testing completion for model {model_name} with prompt: {prompt}")
    
    # Prepare input
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(model.device)
    
    # Log tokenized input
    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    logger.info(f"Tokenized input: {input_tokens}")
    
    # Generate with different parameters
    try:
        # Try with default parameters
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=False,  # Use greedy decoding
                pad_token_id=tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') else None
            )
        
        # Decode the generated text
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract the completion (text after the prompt)
        completion = generated_text[len(prompt):].strip()
        
        # Check if the completion matches the expected completion
        is_match = False
        if expected_completion:
            # Try different matching criteria
            # exact_match = completion == expected_completion
            # case_insensitive_match = completion.lower() == expected_completion.lower()

            contains_match = expected_completion.lower() in completion.lower()
            starts_with_match = completion.lower().startswith(expected_completion.lower())

            is_match = contains_match or starts_with_match
            
            # Log all matching criteria
            match_info = {
                "exact_match": completion.lower() == expected_completion.lower(),                                                                                                       
                "starts_with_match": completion.lower().startswith(expected_completion.lower()),                                                                                        
                "contains_match": completion.lower() in completion.lower()  
            }
        else:
            match_info = {"no_expected_completion_provided": True}
        
        # Log the results
        log_completion_evaluation(
            prompt=prompt,
            expected_completion=expected_completion or "N/A",
            generated_text=generated_text,
            extracted_completion=completion,
            is_match=is_match,
            model_name=model_name,
            additional_info={"match_info": match_info}
        )
        
        return {
            "prompt": prompt,
            "expected_completion": expected_completion,
            "generated_text": generated_text,
            "completion": completion,
            "is_match": is_match,
            "match_info": match_info
        }
        
    except Exception as e:
        logger.error(f"Error testing completion: {e}")
        return {
            "prompt": prompt,
            "error": str(e)
        }

def analyze_tokenizer_behavior(
    tokenizer: Any,
    text: str,
    tokenizer_name: str = "unknown"
) -> Dict[str, Any]:
    """
    Analyze tokenizer behavior for a given text.
    
    Args:
        tokenizer: The tokenizer to analyze
        text: The text to tokenize
        tokenizer_name: Name or identifier of the tokenizer
        
    Returns:
        Dictionary with tokenizer analysis
    """
    logger.info(f"Analyzing tokenizer {tokenizer_name}")
    
    # Check special tokens
    special_tokens = {
        "pad_token": getattr(tokenizer, "pad_token", None),
        "eos_token": getattr(tokenizer, "eos_token", None),
        "bos_token": getattr(tokenizer, "bos_token", None),
        "unk_token": getattr(tokenizer, "unk_token", None),
        "mask_token": getattr(tokenizer, "mask_token", None),
    }
    
    # Tokenize the text
    tokens = tokenizer.tokenize(text)
    token_ids = tokenizer.encode(text)
    decoded = tokenizer.decode(token_ids)
    
    # Check if decoding is reversible
    is_reversible = decoded == text
    
    # Log the results
    logger.info(f"Tokenizer analysis for {tokenizer_name}:")
    logger.info(f"Special tokens: {special_tokens}")
    logger.info(f"Input text: {text}")
    logger.info(f"Tokenized: {tokens}")
    logger.info(f"Token IDs: {token_ids}")
    logger.info(f"Decoded: {decoded}")
    logger.info(f"Reversible: {is_reversible}")
    
    return {
        "tokenizer_name": tokenizer_name,
        "special_tokens": special_tokens,
        "input_text": text,
        "tokens": tokens,
        "token_ids": token_ids,
        "decoded": decoded,
        "is_reversible": is_reversible
    }
