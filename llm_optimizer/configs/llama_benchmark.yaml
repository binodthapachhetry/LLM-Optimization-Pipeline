# Configuration for benchmarking Llama models
benchmark:
  # Model paths
  baseline_model: "meta-llama/Llama-3.2-1B-Instruct"  # Original model
  optimized_model: "ContactDoctor/Bio-Medical-Llama-3-2-1B-CoT-012025"  # Quantized model (will auto-detect GGUF file)
  
  # Output directory
  output_dir: "./outputs/llama_benchmark"
  
  # Benchmark parameters
  sequence_lengths: [128, 256, 512]  # Sequence lengths to test
  batch_sizes: [1, 2, 4]  # Batch sizes to test
  num_iterations: 5  # Number of iterations for each test
  
  # Benchmark types
  benchmark_quality: true  # Whether to benchmark quality
  benchmark_memory: true   # Whether to benchmark memory usage
  
  # Performance benchmarking options
  performance:
    warm_up_iterations: 3  # Number of warm-up iterations before timing
    timeout_seconds: 300   # Maximum time to allow for a single benchmark configuration
    skip_large_configs: true  # Skip configurations that might cause OOM
    
  # Evaluation tasks
  evaluation:
    tasks: ["perplexity"]  # Tasks to evaluate
    dataset: "wikitext"  # Dataset for evaluation
    max_samples: 10  # Maximum number of samples to evaluate
    max_sequence_length: 512
    
  # Reporting options
  reporting:
    include_raw_data: true  # Include raw benchmark data in reports
    generate_plots: true    # Generate visualization plots
    save_model_metadata: true  # Save detailed model information
